---
title: "Chi Squared Tests for Heterogeneity"
author: "Rachael 'Rocky' Aikens, Voight Lab"
date: "June 20, 2017"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

# Set Up

```{r setup, message = FALSE, echo = F}
library(knitr)

# set wd
knitr::opts_knit$set(root.dir = '../../data')

# color vector so that plot colorings match Harris
h.colors <- c("dark blue", "magenta", "purple", "forest green", "#0099FF", "red")

 
# import libraries we need
library(ggplot2)
library(readr)
```

This analysis requires data on the counts of each 3mer, 5mer, and 7mer polymorphism type private to each of the 1,000 genomes nonadmixed continental groups (AFR, EUR, EAS, and SAS). All of this data has been preprocessed using the process_chrom_counts function in "code/data_wrangling/process_chrom_counts.R".  I call these "count dataframes."  An example is shown here:

```{r load_data, echo = F}
# load data
AFR_3mer_counts <- read.delim('3mer/AFR_3mer_counts.txt')
EUR_3mer_counts <- read.delim('3mer/EUR_3mer_counts.txt')
EAS_3mer_counts <- read.delim('3mer/EAS_3mer_counts.txt')
SAS_3mer_counts <- read.delim('3mer/SAS_3mer_counts.txt')

AFR_5mer_counts <- read.delim('5mer/AFR_5mer_counts.txt')
EUR_5mer_counts <- read.delim('5mer/EUR_5mer_counts.txt')
EAS_5mer_counts <- read.delim('5mer/EAS_5mer_counts.txt')
SAS_5mer_counts <- read.delim('5mer/SAS_5mer_counts.txt')

AFR_7mer_counts <- read.delim('7mer/AFR_7mer_counts.txt')
EUR_7mer_counts <- read.delim('7mer/EUR_7mer_counts.txt')
EAS_7mer_counts <- read.delim('7mer/EAS_7mer_counts.txt')
SAS_7mer_counts <- read.delim('7mer/SAS_7mer_counts.txt')

# example of 7mer dataset
head(AFR_5mer_counts[,1:8], n = 5)
```

********************************************************************************

\pagebreak

# Pairwise Chi Squared Tests

This section details how to perform the pairwise chi squared tests from Harris 2015.  These steps were mostly used for replication.

## Methodology

Two R functions that I use in this analysis:

* **pairwise.chi** Given two count dataframes, output a dataframe of chi-squared test results for each context.  The arguement 'filter' (set by default to be true), will output "NA" as the p-value for any test for which the chi squared assumptions may not be correct.

* **volcano.plot** Given two count dataframes, plus the output from pairwise.chi, construct a volcano plot as in Harris 2015.  Also takes the arguement lab.lim, which determines the lower p-value limit for which polymorphisms types should be labeled.

The code used to definte pairwise.chi is shown below:

```{r pairwise.chi}
# calculates homogeneity test p values for pairwise comparisons of two dfs of counts
pairwise.chi <- function(counts.1, counts.2, filter = T){
  n.contexts = length(counts.1$Context)
  result <- data.frame(matrix(ncol=2,nrow=n.contexts))
  colnames(result) <- c("Context", "p")
  
  result$Context <- counts.1$Context
  sum.1 <- sum(counts.1$Count)
  sum.2 <- sum(counts.2$Count)

  for (i in 1:n.contexts){
    c.a <- c(counts.1$Count[i], counts.2$Count[i])
    c.b <- c(sum.1, sum.2) - c.a
    data <- cbind(c.a, c.b)
    warning <- is(tryCatch(chisq.test(data), warning = function(w) w), "warning")
    if (filter == T & warning){
      result$p[i] <- NA
    }
    else result$p[i] <- chisq.test(data)$p.value
  }
  
  return(result)
}
```

```{r, volcano.plot, echo = F}
# volcano plot as in Harris 2015
# plot parameters often require some tinkering to come out well
volcano.plot <- function(counts.1, counts.2, p.vals, lab.lim){
  p.vals <- p.vals[complete.cases(p.vals),]
  f.1 <- counts.1$Count/sum(counts.1$Count)
  f.2 <- counts.2$Count/sum(counts.2$Count)
  f.diff <- (f.1 -f.2)/f.2
  alpha <- 0.05/(6*length(counts.1$Count))
  
  sigs <- subset(counts.1, p.vals$p < lab.lim)
  f.diff.sigs <- subset(f.diff, p.vals$p < lab.lim)
  p.sigs <- subset(p.vals, p.vals$p < lab.lim)
  
  v_plot <- qplot(f.diff, -log10(p.vals$p)) +
    labs(x = "Fold excess in Europe", y = expression(-log[10](italic(p))),
         title = "Europe v. Africa")+
    scale_color_manual("", values = h.colors)+ # set colors
    geom_text(data = sigs, 
              aes(f.diff.sigs, -log10(p.sigs$p), label = sigs$Context, color = sigs$X1mer), 
              vjust =0, nudge_y =.5, size = 3, check_overlap = TRUE)+ # label highly significant
    geom_point(aes(color=factor(counts.1$X1mer)), size = 3)+ # Color pts by one_mer
    geom_hline(yintercept = -log10(alpha), color = "black", linetype = 2)+ # significance line
    geom_text(aes(.45, -log10(alpha)+3, label = "significant (p < 9e-5)", vjust = 0),
              color = "black", size = 5)+ # significant text
    theme(axis.text.x = element_text(size = rel(1.3)), 
          axis.title.x = element_text(size = rel(1.3)), # adjust text sizes
          axis.title.y = element_text(size = rel(1.3)), 
          axis.text.y = element_text(size = rel(1.35)), 
          legend.text = element_text(size = rel(1.2)), legend.position = c(0.1, 0.7),
          title = element_text(size = rel(1.2))) # legend position and size
  
  return(v_plot)
}
```

\pagebreak

## Examples

As previously mentioned, these methods are not central to my analysis and mostly important for replication. It's worthwhile to note that these functions and data replicate volcano plots like the ones from Kelley Harris's leading figure in PNAS 2015:

```{r volcano plot example, echo = F}
p.EURvAFR <- pairwise.chi(EUR_3mer_counts, AFR_3mer_counts)
volcano.plot(EUR_3mer_counts, AFR_3mer_counts, p.EURvAFR, 1e-50)
```

********************************************************************************

\pagebreak

# Fourway Tests for Homogeneity

In order to lighten the multiple hypothesis testing burden of running ${4 \choose 2} = 6$ pairwise comparisons for each possible polymorphism type, we switched to a homogeneity testing framework, which helps us rank polymorphism types based on how much they vary between populations.  This is the dominant analysis technique we use to identify polymorphisms which are heterogeneous across continental groups.

## Methodology

This section defines one function for calculation and two for visualization. Again, I'm hiding the code for the plotting functions in the compiled report because it's not essential to understanding.

* **fourway.chi** Given four count dataframes, output a dataframe of chi-squared test results for each context.

* **hom.test.plot** Given the output from fourway.chi, construct a volcano plot as in Harris 2015. Also takes the arguement lab.lim, which determines the lower p-value limit for which polymorphisms types should be labeled, and the boolean, NoTCC, which, when True, leaves out labels for any polymorphism with the 3mer subcontext TCC->T.

* **sigs.plot** Given the same arguments as hom.test.plot, make a plot of just the significant results.

The r code used to define fourway.chi is shown below:

```{r fourway.chi}
# calculates homogeneity test p values for Fourway comparisons of counts dfs
fourway.chi <- function(AFR, EUR, EAS, SAS, filter = T){
  n.contexts = length(AFR$Context)
  
  # make dataframe for results
  result <- data.frame(matrix(ncol=9,nrow=n.contexts))
  colnames(result) <- c("Context", "X5mer","X3mer", "X1mer", 
                        "AFR.Count", "EUR.Count", "EAS.Count", "SAS.Count", "p")
  result$Context <- AFR$Context
  result$X5mer <- AFR$X5mer # for smaller contexts, X3mer and X5mer columns do not exist,
  result$X3mer <- AFR$X3mer # and will disappear at this step
  result$X1mer <- AFR$X1mer 
  result$AFR.Count <- AFR$Count; result$EUR.Count <- EUR$Count
  result$EAS.Count <- EAS$Count; result$SAS.Count <- SAS$Count
  
  # start setting up tables
  sums <- c(sum(AFR$Count), sum(EUR$Count), sum(EAS$Count), sum(SAS$Count))
  
  # set up table and run test for each context
  for (i in 1:n.contexts){
    c.a <- c(AFR$Count[i], EUR$Count[i], EAS$Count[i], SAS$Count[i])
    c.b <- sums - c.a
    data <- cbind(c.a, c.b)
    warning <- is(tryCatch(chisq.test(data), warning = function(w) w), "warning")
    if (filter == T & warning){
      result$p[i] <- NA}
    else result$p[i] <- chisq.test(data)$p.value
    }
  return(result)
}
```

```{r hom.plots, echo = F}
# p-value scattter plot for 4-way comparison
hom.test.plot <- function(p.values, lab.lim, NoTCC = F){
  p.values <- p.values[complete.cases(p.values),]
  n = length(p.values$Context)
  alpha = 0.05/n
  
  # preprocessing data
  data <- p.values[order(p.values$p),]
  data$Context <- as.character(data$Context)
  data$Context <- factor(data$Context, levels = unique(data$Context))
  
  # vector to decide which points to label
  include <- (data$p < lab.lim)
  if (NoTCC){
    include <- include & (data$X3mer != "TCC->T")
  }
  sigs <- subset(data, include)
  
  # plot
  my.plot <- qplot(data$Context, -log10(data$p)) +
    labs(x = "Polymorphism type", y = expression(-log[10](italic(p))))+
    scale_color_manual("", values = h.colors)+
    geom_point(aes(color=factor(data$X1mer)), size = 2.5)+
    geom_hline(yintercept = -log10(alpha), color = "black", linetype = 2, size = 1)+
    geom_text(data = sigs, aes(sigs$Context, -log10(sigs$p), 
                               label = sigs$Context, color = sigs$X1mer), 
              hjust = 0, nudge_x = 1.5, nudge_y = 2, size = 4, check_overlap = TRUE)+
    geom_text(aes(15, -log10(alpha)-2, label = paste("significant, p < ", signif(alpha, 1)), vjust = 1),
              color = "black", size = 4)+
    theme(axis.ticks.x = element_blank(),
          axis.text.x = element_blank(), axis.title.x = element_text(size = rel(1.5)),
          axis.title.y = element_text(size = rel(1.5)), axis.text.y = element_text(size = rel(1.4)),
          legend.text = element_text(size = rel(1.25)), legend.position = c(0.8, 0.8))
  
  return(my.plot)
}


# p-value scattter plot for 4-way comparison which just shows significant results
sigs.plot <- function(p.vals, lab.lim, NoTCC = F){
  p.vals <- p.vals[complete.cases(p.vals),]
  n = length(p.vals$Context)
  alpha = 0.05/n

  # preprocessing data
  p.values <- subset(p.vals, p.vals$p < alpha)
  data <- p.values[order(p.values$p),]
  data$Context <- as.character(data$Context)
  data$Context <- factor(data$Context, levels = unique(data$Context))
  
  
  # vector to decide which points to label
  include <- (data$p < lab.lim)
  if (NoTCC){
    include <- include & (data$X3mer != "TCC->T")
  }
  sigs <- subset(data, include)
  
  my.plot <- qplot(data$Context, -log10(data$p)) +
    labs(x = "Polymorphism type", y = expression(-log[10](italic(p))))+
    scale_color_manual("", values = h.colors)+
    geom_point(aes(color=factor(data$X1mer)), size = 1.75)+
    geom_text(data = sigs, aes(sigs$Context, -log10(sigs$p), 
                               label = sigs$Context, color = sigs$X1mer), 
              hjust = 0, nudge_x = 2, nudge_y = .5, size = 2.5, check_overlap = TRUE) +
    theme(axis.ticks.x = element_blank(), 
          axis.text.x = element_blank(), axis.title.x = element_text(size = rel(1.5)),
          axis.title.y = element_text(size = rel(1.5)), axis.text.y = element_text(size = rel(1.4)),
          legend.text = element_text(size = rel(1.25)), legend.position = c(0.8, 0.8))
  
  return(my.plot)
}
```

\pagebreak

## 3mers

Now using these functions, we can run the following tests for 3mer polymorphism types which are heterogeneous across ancestral groups.  We can begin with the 3mer context paradigm, which is most commonly used in the literature in this area.  The table below shows all 3mer polymorphism types, ranked according to their p value in a fourway.chi test for heterogeneity across populations.

```{r 3mer homogeneity, echo = F}
# calculate chi squared p values
p.3mer <- fourway.chi(AFR_3mer_counts, EUR_3mer_counts, EAS_3mer_counts, SAS_3mer_counts)

# plot
hom.test.plot(p.3mer, 1E-50)

# number significant
ns.3 <- sum(p.3mer$p < 0.05/96)

# most significant results
kable(head(p.3mer[order(p.3mer$p),], 10), row.names = F, digits = 600,
      caption = "10 most significant 3mer polymorphisms")
```

This plot highlights the top six contexts, which are significant at p<1e-60.  They include TCC->T, ACC->T, and TCT->T, which have been previously reported as part of a European signal of C->T elevation.  The next three contexts have not been noted by any previous analyses of mutation rate heterogeneity.  There are `r ns.3` significant polymorphisms falling out from this analysis after Bonferroni correction.

\pagebreak

## 5mers

Now we move to higher levels of sequence context, which may capture more detail in how mutation rates vary.  In this section, we run the same analysis as above for 5mers, identifying variable polymorphism types which may not have been highlighted at the 3mer level.  

```{r 5mer homogeneity test, echo = F}
# calculate chi squared p values
p.5mer <- fourway.chi(AFR_5mer_counts, EUR_5mer_counts, EAS_5mer_counts, SAS_5mer_counts)

# extract significantly variable mutation types and order by p value
p.5mer <- p.5mer[complete.cases(p.5mer),]
alpha <- 0.05/length(p.5mer$Context)
sig.5mer <- subset(p.5mer, p.5mer$p < alpha)
sig.5mer <- sig.5mer[order(sig.5mer$p),]

# number of significant polymorphism types
ns.5 <- length(sig.5mer$Context)
```

The plot below shows the homogeneity test p values for just the `r ns.5` 5mers which are significant after bonferroni correction.

```{r 5mer homogeneity plot, echo = F}
# plot
sigs.plot(p.5mer, 1E-25, NoTCC = T)

# filter types from TCC->T signal
EUR.signal <- c("TCC->T", "ACC->T", "TCT->T", "CCC->T")
new.signal <- c("GAT->T", "ACC->A", "GAC->T", "ACG->T", "CCG->T", "GCG->T", "TCG->T")

new.5mers <- subset(sig.5mer, !is.element(sig.5mer$X3mer, c(EUR.signal, new.signal)))

# count number of hits left over
nn.5 <- length(new.5mers$Context)
```

It is clear from this plot (and the one for 7mers) that many of the signficant polymorphisms at the 5mer and 7mer level are a part of the signal of C->T elevation that we observe at the 3mer level.  This begs the question: how many significant 5mer signals are there outside of the 3mer subcontexts we have already idenified?  To answer this question, I removed from the significant 5mer set all mutations whose 3mer subcontexts correspond to the European C->T elevation (TCC->T, ACC->T, TCT->T, and CCC->T), or otherwise highlighted by the heatmap signals 1-3 we identify in another analysis (see heatmaps report) identified in the previous section. This leaves a total of `r nn.5` new significant polymorphisms.  The following table shows the most highly significant 5mers *outside* of these 3mer signals that have already been noted:

```{r 5mer print table, echo = F}
# show top results
kable(head(new.5mers[,-c(2,3)], 10), row.names = F, digits = 600,
      caption = "10 most significant 5mers not noted on a 3mer level")
```

Note that the most highly significant new 5mer is TTAAA->T, which corresponds to the 8th most significant 3mer, TAA->T.  As we will see, the 7mer TTTAAAA->T is also one of the top significantly variable 7mers.

\pagebreak 
 
## 7mers

Now we move to the same analysis at the 7mer level, beginning with a homogeneity test plot:

```{r 7mer homogeneity, warning = F, echo = F}
# calculate chi squared p values
p.7mer <- fourway.chi(AFR_7mer_counts, EUR_7mer_counts, EAS_7mer_counts, SAS_7mer_counts)

# plot
sigs.plot(p.7mer, 1E-10, NoTCC = T)

# extract significantly variable mutation types and order by p value
p.7mer <- p.7mer[complete.cases(p.7mer),]
alpha <- 0.05/length(p.7mer$Context)
sig.7mer <- subset(p.7mer, p.7mer$p < alpha)
sig.7mer <- sig.7mer[order(sig.7mer$p),]

# number of significant polymorphism types
ns.7 <- length(sig.7mer$Context)

# filter types from known signals
new.7mers <- subset(sig.7mer, !is.element(sig.7mer$X3mer, c(EUR.signal, new.signal)))

# count number of hits left over
nn.7 <- length(new.7mers$Context)
```

The plot above shows heterogeneity test p values for the `r ns.7` 7mers signifcant after bonferroni correction. We can ask the same question about these results as we did with the 5mers: which of these 7mers are results that we have not previously picked out from our 3mer analysis?  Filtering these signals leaves `r nn.7` significant results, the top ten of which are shown below:

```{r 7mer signif table, echo = F}
# show top results
kable(head(new.7mers[,-c(2,3,4)], 10), row.names = F, digits = 600,
      caption = "10 most significant 7mers not noted on a 3mer level")
```

## Summary

The following table summarizes the numbers of significant results from this section.

 Context Model | Number Significant | Number New
---------------|--------------------|-------------
 3mer          |  `r ns.3`          |      --
 5mer          |  `r ns.5`          |  `r nn.5`
 7mer          |  `r ns.7`          |  `r nn.7`
 
********************************************************************************

\pagebreak

# False Discovery Rate Corrections

All of the tests in the above section use the Bonferroni Correction, which is conservative even when hypothesis tests are positively correlated (as is most-likely the case here.)  However, the Bonferroni correction is often criticized as being *too* conservative.  For these reasons, it may be useful to apply other significance thresholds which account for the multiple testing burden.

## Methodology

Initially, I tried to use the qvalue package to perform false discovery rate analysis.  However, this package proved difficult to use, since our p-values from our homogeneity tests don't follow a uniform [0,1] distribution (they range from 0-0.45).  Instead, I decided to use the built-in R function, p.adjust(), which uses Benjamini-Hoochberg-Yekutieli.  These methods should be acceptable even when the p-values are positively correlated.  The following function, **fdr**, performs simple fdr analysis on an output dataframe from a chi-squared function.

```{r fdr}
fdr <- function(p.data){
  p.data <- p.data[complete.cases(p.data),]
  
  # This uses Benjamini-Hochberg-Yekutieli fdr
  p.data$fdr <- p.adjust(p.data$p, method = "fdr")
  
  # multiple hypothesis correction by holm
  p.data$holm <- p.adjust(p.data$p, method = "holm")
  
  alpha = 0.05/length(p.data$p)
  p.data <- p.data[complete.cases(p.data), ]
  
  n.sig <- c(length(p.data$p), sum(p.data$p < alpha), sum(p.data$holm< 0.05), 
             sum(p.data$fdr< 0.1), sum(p.data$fdr< 0.05), 
             sum(p.data$fdr< 0.01), sum(p.data$fdr< 0.001))
  
  names(n.sig) <- c("Total tests","Bonferroni", "Holm", 
                    "FDR<0.1", "FDR<0.05", 
                    "FDR<0.01", "FDR<0.001")
  
  return(list(n.sig, p.data))
}
```

I am additionally defining the funtion **qq.labels**, which takes in a p-value dataframe, a lab.lim, a title (default = "Quantile-quantile plot of p-values"), and the NoTCC arguement and returns a qq plot of all contexts, color-coded and labeled.  In the following section, I will construct qq plots and run fdr analysis for each of the 3mer, 5mer, and 7mer models.

```{r qq.labels, echo = F}
qq.labels <- function(pdata, lab.lim, title="Quantile-quantile plot of p-values", NoTCC = F) {

  # preprocess data to order and remove nas 
  pdata <- pdata[complete.cases(pdata$p),]
  pdata <- pdata[order(pdata$p),]
  pdata$Context <- as.character(pdata$Context)
  pdata$Context <- factor(pdata$Context, levels = unique(pdata$Context))
  
  # vector to record which SNPs to label
  include <- (pdata$p < lab.lim)
  if (NoTCC){
    include <- include & (pdata$X3mer != "TCC->T")
  }
  sigs <- subset(pdata, include)
  
  # x and y coordinates in qq plot
  o <- -log10(sort(pdata$p,decreasing=F))
  e <- -log10( 1:length(o)/length(o) )
  
  
  # plot
  plot <- qplot(e,o, xlim=c(0,max(e)), ylim=c(0,max(o)))+
    scale_x_continuous(name=expression(Expected~~-log[10](italic(p))))+
    scale_y_continuous(name=expression(Observed~~-log[10](italic(p))))+
    scale_color_manual("", values = h.colors)+
    geom_point(aes(color=factor(pdata$X1mer)), size = 1.75)+
    geom_text(data = sigs, 
              aes(subset(e, include), subset(o, include), label = sigs$Context, color = sigs$X1mer), 
              hjust = 1, nudge_x = -0.03, nudge_y = .5, size = 3, check_overlap = T)+
    labs(title = title)+
    theme(axis.text.x = element_text(size = rel(1.3)), axis.title.x = element_text(size = rel(1.3)), # adjust text sizes
          axis.title.y = element_text(size = rel(1.3)), axis.text.y = element_text(size = rel(1.35)), 
          legend.text = element_text(size = rel(1.2)),
          title = element_text(size = rel(1.2))) +
    geom_abline(intercept=0,slope=1, col="red")
  
  return(plot)
}
```

\pagebreak

## 3mers

```{r 3mer fdr, warning=FALSE, message = FALSE, echo = F}
# construct qq plot for p-values
qq.labels(pdata = p.3mer, lab.lim = 1e-50, NoTCC = F)

# fdr analysis for 3mers
a <- fdr(p.3mer)
fdr.3mer <- a[2]
sig.3mer <- a[1];sig.3mer
```

The qq plots shown above display relatively the same information as the p-value plots by context.  However, it is worth noting that the observed p values, even at the lower end, are above expected p-value quantiles.  This may suggest that in fact, every context is significant so that the null distribution of p values does not hold.  More realistically, this appears to be an artifact of the fact that hypothesis tests set up as above are actually positively correlated (that is, a small p-value in one test probably increases the likelihood of a small p-value in another test).

One possible solution to this problem would be to simulate null-distributed datasets to approximate an emprical distribution for expected p-value.

\pagebreak

## 5mers

```{r 5mer fdr, warning=FALSE, message = FALSE, echo = F}
# construct qq plot for p-values
qq.labels(pdata = p.5mer, lab.lim = 1e-15, NoTCC = T)

# fdr analysis for 5mers
a <- fdr(p.5mer)
fdr.5mer <- a[2]
sig.5mer <- a[1];sig.5mer
```

\pagebreak

## 7mers
```{r 7mer fdr, warning=FALSE, message = FALSE, echo = F}
# construct qq plot for p-values
qq.labels(pdata = p.7mer, lab.lim = 1e-10, NoTCC = T)

# fdr analysis for 7mers
a <- fdr(p.7mer)
fdr.7mer <- a[2]
sig.7mer <- a[1];sig.7mer
```

********************************************************************************

\pagebreak

# Ordered p-values

## Methodology

The following function, **ordered.p**, returns a p-value dataframe with p calculated based on the methods from Harris and Pritchard, 2017.  This method is proven to give less-significant results, but helps partially combat the problem of positive correlation between p values using our original methods.

```{r ordered.p}
ordered.p <- function(pdata){
  #preprocess data to order and remove nas 
  pdata <- pdata[complete.cases(pdata$p),]
  myorder <- order(pdata$p)
  n.muts <- length(pdata$p)
  
  p.ordered <- rep(0, n.muts)
  
  #set largest p-value 
  j <- myorder[n.muts]
  p.ordered[j] <- pdata$p[j]
  
  #initialize not mutated counts based on this lowest p-value mutation
  not.mut <- c(pdata$AFR.Count[j], pdata$EUR.Count[j], 
               pdata$EAS.Count[j], pdata$SAS.Count[j])
  
  for (i in n.muts:1){
    j <- myorder[i]
    mut <-c(pdata$AFR.Count[j], pdata$EUR.Count[j], 
            pdata$EAS.Count[j], pdata$SAS.Count[j])
    data <- cbind(mut, not.mut)
    p.ordered[j] <- chisq.test(data)$p.value
    
    #add these mutations to the not.mutated counts for future tests
    not.mut <- not.mut + mut
  }
  pdata$p <- p.ordered
  return(pdata)
}
```

In the following sections, I will repeat all of the above analyses for fourway homogeneity test in terms of ordered p value.

\pagebreak

## 3mers

```{r 3mer op, echo = F}
# calculate ordered p values
o.p.3mer <- ordered.p(p.3mer)

# number significant
o.ns.3 <- sum(o.p.3mer$p < 0.05/96)

# plot significant 3mers and print most signif. results
hom.test.plot(o.p.3mer, 1e-60)
kable(head(o.p.3mer[order(o.p.3mer$p),], 10), row.names = F, digits  = 600,
      caption = "10 most significant 3mers using ordered p value correction")
```

Notice that, as before, GAT->T and ACC->A are the 4th and 5th most significant results.  Meanwhile, ACA->T, the third highly significant signal from earlier, is moved from the 6th to the 9th place in terms of significance, and no longer sticks out from the remaining mutation types as it once did. Certain mutations (for example TAA->T) have dropped in significance notably (from 8th to 17th), while C->T mutations seem to be featured much more prominently among the most significant polymorphism types.

```{r 3mer op fdr, message = F, echo = F}
# qqplot
qq.labels(o.p.3mer, 1e-60)

# fdr analysis for 3mers
a <- fdr(o.p.3mer)
o.fdr.3mer <- a[2]
o.sig.3mer <- a[1];o.sig.3mer
```

\pagebreak

## 5mers

```{r 5mer op, echo = F}
# calculate ordered p values
o.p.5mer <- ordered.p(p.5mer)

# extract significantly variable mutation types and order by p value
alpha <- 0.05/length(o.p.5mer$Context)
o.sig.5mer <- subset(o.p.5mer, o.p.5mer$p < alpha)
o.sig.5mer <- o.sig.5mer[order(o.sig.5mer$p),]

# number of significant polymorphism types
o.ns.5 <- length(o.sig.5mer$Context); o.ns.5

# filter types from known 3mer signals
o.new.5mers <- subset(o.sig.5mer, !is.element(o.sig.5mer$X3mer, c(EUR.signal, new.signal)))

# count number of hits left over
o.nn.5 <- length(o.new.5mers$Context); o.nn.5

# plot significant 5mers and print most signif. new results
sigs.plot(o.p.5mer, 1e-15, NoTCC = T)
kable(head(o.new.5mers[,-c(2,3)], 10), row.names = F, digits = 600,
      caption = "10 most significant new 5mers using ordered p value correction")
```

Notice that, as before, TTAAA->T is the most signifcant new 5mer, with a p-value several orders of magnitude smaller than the other new significant 5mers.  Again, many of the top results are present in a different order than in the original test.

```{r 5mer op fdr, message = F, echo = F}
# qqplot
qq.labels(o.p.5mer, 1e-15, NoTCC = T)

# fdr analysis for 3mers
a <- fdr(o.p.5mer)
o.fdr.5mer <- a[2]
o.sig.5mer <- a[1];o.sig.5mer
```

\pagebreak

## 7mers

```{r 7mer op, echo = F}
# calculate ordered p values
o.p.7mer <- ordered.p(p.7mer)

# extract significantly variable mutation types and order by p value
alpha <- 0.05/length(o.p.7mer$Context)
o.sig.7mer <- subset(o.p.7mer, o.p.7mer$p < alpha)
o.sig.7mer <- o.sig.7mer[order(o.sig.7mer$p),]

# number of significant polymorphism types
o.ns.7 <- length(o.sig.7mer$Context)
# filter types from known 3mer signals
o.new.7mers <- subset(o.sig.7mer, !is.element(o.sig.7mer$X3mer, c(EUR.signal, new.signal)))

# count number of hits left over
o.nn.7 <- length(o.new.7mers$Context)

# plot significant 7mers and print most signif. new results
sigs.plot(o.p.7mer, 1e-10, NoTCC = T)
kable(head(o.new.7mers[,-c(2,3,4)], 10), digits = 600, row.names = F,
      caption = "10 most significant new 7mers using ordered p value correction")
```

Notice that, for 7mers, the ordering of the top ten most significant results is entirely unchanged.

```{r 7mer op fdr, message = F, echo = F}
# qqplot
qq.labels(o.p.7mer, 1e-10, NoTCC = T)

# fdr analysis for 7mers
a <- fdr(o.p.7mer)
o.fdr.7mer <- a[2]
o.sig.7mer <- a[1];o.sig.7mer
```


## Summary

The following table summarizes the numbers of significant results from this section.

 Context Model | Number Significant | Number New
---------------|--------------------|-------------
 3mer          |  `r o.ns.3`        |      --
 5mer          |  `r o.ns.5`        |  `r o.nn.5`
 7mer          |  `r o.ns.7`        |  `r o.nn.7`
 
 Notice that using the ordered p value calculation causes us to pick up far fewer significant 3mers, but slightly more 5mers and 7mers.  Moreover, it seems that using p ordered on 3mers has a much greater effect than on 5mers and 7mers.  This might be expected because, for the most significant 7mers or 5mers, our p-value calculations still include most of the data, however, for our highly significant 3mers, a much larger portion of the data is excluded.
 
```{r save data, echo = F, eval = T}
# this is just some helper code that I can run to re-save the results from this analysis

p.3mer <- p.3mer[order(p.3mer$p),]
sig.3mers <- subset(p.3mer, p.3mer$p < 0.05/96)

setwd("../workflows_finished/chi_squared_tests/results/")

# significant results
write.table(sig.3mers, "unordered_p/significant_3mers.txt", quote = F, sep = "\t", row.names = F)
write.table(new.5mers, "unordered_p/significant_new_5mers.txt", quote = F, sep = "\t", row.names = F)
write.table(new.7mers, "unordered_p/significant_new_7mers.txt", quote = F, sep = "\t", row.names = F)

# fdr
write.table(fdr.3mer, "unordered_p/fdr_3mers.txt", quote = F, sep = "\t", row.names = F)
write.table(fdr.5mer, "unordered_p/fdr_5mers.txt", quote = F, sep = "\t", row.names = F)
write.table(fdr.7mer, "unordered_p/fdr_7mers.txt", quote = F, sep = "\t", row.names = F)

o.p.3mer <- o.p.3mer[order(o.p.3mer$p),]
o.sig.3mers <- subset(o.p.3mer, o.p.3mer$p < 0.05/96)

# significant results
write.table(o.sig.3mers, "ordered_p/o_significant_3mers.txt", quote = F, sep = "\t", row.names = F)
write.table(o.new.5mers, "ordered_p/o_significant_new_5mers.txt", quote = F, sep = "\t", row.names = F)
write.table(o.new.7mers, "ordered_p/o_significant_new_7mers.txt", quote = F, sep = "\t", row.names = F)

# fdr
write.table(o.fdr.3mer, "ordered_p/o_fdr_3mers.txt", quote = F, sep = "\t", row.names = F)
write.table(o.fdr.5mer, "ordered_p/o_fdr_5mers.txt", quote = F, sep = "\t", row.names = F)
write.table(o.fdr.7mer, "ordered_p/o_fdr_7mers.txt", quote = F, sep = "\t", row.names = F)
```
